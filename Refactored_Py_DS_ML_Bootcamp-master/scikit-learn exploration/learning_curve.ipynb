{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the learning_curve function works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the following article: https://medium.com/@nesrine.ammar/how-learning-curve-function-from-scikit-learn-works-692d7d566d17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=10000, n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.utils import check_random_state\n",
    "cv = 5\n",
    "estimator= LogisticRegression()\n",
    "cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
    "cv_iter = list(cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_classifier(estimator) # returns bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1942 1945 1946 ... 9997 9998 9999] len:  8000\n",
      "test: [   0    1    2 ... 2058 2062 2063] len:  2000\n",
      "train: [   0    1    2 ... 9997 9998 9999] len:  8000\n",
      "test: [1942 1945 1946 ... 4032 4035 4037] len:  2000\n",
      "train: [   0    1    2 ... 9997 9998 9999] len:  8000\n",
      "test: [3955 3962 3963 ... 6049 6051 6053] len:  2000\n",
      "train: [   0    1    2 ... 9997 9998 9999] len:  8000\n",
      "test: [5945 5948 5951 ... 8009 8013 8015] len:  2000\n",
      "train: [   0    1    2 ... 8009 8013 8015] len:  8000\n",
      "test: [7990 7991 7993 ... 9997 9998 9999] len:  2000\n"
     ]
    }
   ],
   "source": [
    "#indices\n",
    "for fold in cv_iter:\n",
    "    print('train:', fold[0],'len: ', len(fold[0]))\n",
    "    print('test:', fold[1], 'len: ', len(fold[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "estimator = LogisticRegression(),\n",
    "X = X, y = y, train_sizes = [100, 1000, 1500], cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 5 splits, an estimator is trained for every training set size specified.\n",
    "\n",
    "training set accuracy drops as sample sizes increase. This indicates that overfitting is being reduced as small sample sizes are easy to overfit. In contrast, accuracy increases for larger test sets.\n",
    "\n",
    "\n",
    "Below we see: Each column in the two arrays designates a split(fold), and each row corresponds to a set size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sizes  [ 100 1000 1500]\n",
      "[[0.97       0.97       0.97       0.97       0.97      ]\n",
      " [0.894      0.897      0.897      0.897      0.897     ]\n",
      " [0.886      0.88866667 0.88866667 0.88866667 0.88866667]]\n",
      "[[0.7785 0.8585 0.8405 0.834  0.831 ]\n",
      " [0.8865 0.8805 0.8695 0.868  0.856 ]\n",
      " [0.887  0.882  0.877  0.8675 0.86  ]]\n"
     ]
    }
   ],
   "source": [
    "print('training sizes ',train_sizes)\n",
    "print(train_scores)\n",
    "print(validation_scores)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = train_scores.mean(axis = 1)\n",
    "validation_scores_mean = validation_scores.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training average:  [0.97       0.8964     0.88813333]\n",
      "testing average:  [0.8285 0.8721 0.8747]\n"
     ]
    }
   ],
   "source": [
    "#average across teh 5 folds\n",
    "#We get an average for each size.\n",
    "print('training average: ', train_scores_mean)\n",
    "print('testing average: ',validation_scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sizes_abs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5bd372032a8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_test_proportions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mn_train_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sizes_abs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_test_proportions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sizes_abs' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_proportions = []\n",
    "for train, test in cv_iter:\n",
    "    for n_train_samples in train_sizes_abs:\n",
    "        train_test_proportions.append((train[:n_train_samples], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
